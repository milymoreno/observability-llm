# ğŸ“˜ **Workshop Plan â€“ IA Generativa para DevOps y Observabilidad Cloud-Native**

**DuraciÃ³n:** 90 minutos

**Formato:** Workshop prÃ¡ctico

**Tema:** AI, ML & Data in Cloud Native

**Requerimientos:** Docker, kubectl, VS Code, Python 3.10+ o Node 18+, Ollama o API de modelo externo

**Fecha sugerida:** SÃ¡bado 29

---

## ğŸŸ¦ 1. Objetivo del Workshop

Este workshop enseÃ±a cÃ³mo integrar modelos de lenguaje (LLMs) con un stack de observabilidad Cloud-Native (Prometheus, Loki, Tempo, Grafana) para generar:

* Explicaciones automÃ¡ticas de incidentes
* Alertas inteligentes
* Recomendaciones proactivas
* ResÃºmenes en lenguaje natural desde logs, mÃ©tricas y trazas

Al finalizar, cada asistente tendrÃ¡ **una mini-demo funcional** capaz de analizar eventos de un sistema y producir insights automatizados.

---

## ğŸŸ© 2. Plan de Trabajo Completo (PreparaciÃ³n del Workshop)

### ğŸ”µ **FASE 1 â€” DiseÃ±o & OrganizaciÃ³n**

1. Definir alcance del workshop y flujo principal.
2. Crear estructura inicial de presentaciÃ³n.
3. Seleccionar herramientas del stack observability.
4. Elegir lenguaje para la demo (Python recomendado).

---

### ğŸŸ© **FASE 2 â€” PreparaciÃ³n del Entorno Local**

1. Instalar herramientas base:
   * Docker
   * kubectl
   * Kind o Minikube
   * Python 3.10+
2. Crear cluster local con Kind.
3. Instalar Prometheus, Loki y Grafana (Helm o manifiesto).
4. Validar conexiones y dashboards bÃ¡sicos.

---

### ğŸŸ§ **FASE 3 â€” Dataset Simulado y Dashboards**

1. Crear dataset de logs y mÃ©tricas simuladas.
2. Crear dashboards en Grafana:
   * Logs
   * CPU/Latencia
   * Alertas
3. Exportar dashboards en JSON para compartir.

---

### ğŸŸ¥ **FASE 4 â€” CreaciÃ³n del Agente LLM**

1. Instalar y validar Ollama (modelo mistral/llama3).
2. Crear script `insight_agent.py` con:
   * Entrada de logs
   * InterpretaciÃ³n del LLM
   * GeneraciÃ³n de explicaciÃ³n + causa + mitigaciÃ³n
3. IntegraciÃ³n con Loki vÃ­a API.
4. IntegraciÃ³n opcional con Alertmanager vÃ­a webhook.

---

### ğŸŸ« **FASE 5 â€” IntegraciÃ³n Completa**

Unir el flujo:

<pre class="overflow-visible!" data-start="2347" data-end="2421"><div class="contain-inline-size rounded-2xl relative bg-token-sidebar-surface-primary"></div></pre>

<pre class="overflow-visible!" data-start="2347" data-end="2421"><div class="contain-inline-size rounded-2xl relative bg-token-sidebar-surface-primary"><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre!"><span><span>Prometheus</span><span> / Loki / Tempo â†’ Script Python â†’ LLM â†’ Insight generado
</span></span></code></div></div></pre>

Pruebas:

* Logs con errores
* Picos de CPU
* Trazas lentas
* Eventos simulados

---

### ğŸŸª **FASE 6 â€” PreparaciÃ³n de la PresentaciÃ³n**

Estructura recomendada:

* IntroducciÃ³n a LLM para DevOps
* Arquitectura Cloud-Native + Generative AI
* Casos reales
* Demo
* Buenas prÃ¡cticas: privacidad, seguridad, filtrado
* Recursos finales

---

### ğŸŸ« **FASE 7 â€” Pruebas Finales del Workshop**

* Instalar cluster desde cero
* Reinstalar stack observability
* Probar agente LLM
* Validar estabilidad en vivo
* Revisar tiempos de respuesta
* Probar plan B con dataset offline

---

### ğŸŸ¦ **FASE 8 â€” Materiales para Participantes**

Incluye:

* Repositorio GitHub
* Manual PDF de instrucciones
* Dataset de logs y mÃ©tricas
* Dashboards JSON
* Script base del agente
* Prompts recomendados
* Bonus: ejemplos de pipelines

---

### ğŸŸ© **FASE 9 â€” EjecuciÃ³n del Workshop**

Checklist para el dÃ­a del evento:

* Verificar Docker y cluster
* Cargar dashboards
* Validar Ollama y API keys
* Tener abiertos: VS Code, terminal, Grafana
* Preparar una demo corta y estable
* Tener un dataset offline por si falla el cluster
* Cerrar con Q&A + recursos adicionales

---

## ğŸ“¦ Archivos iniciales sugeridos para el repositorio

<pre class="overflow-visible!" data-start="3686" data-end="4109"><div class="contain-inline-size rounded-2xl relative bg-token-sidebar-surface-primary"></div></pre>

<pre class="overflow-visible!" data-start="3686" data-end="4109"><div class="contain-inline-size rounded-2xl relative bg-token-sidebar-surface-primary"><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre!"><span><span>/workshop/
  â”œâ”€â”€ README.md
  â”œâ”€â”€ plan_trabajo.md   â† ESTE DOCUMENTO
  â”œâ”€â”€ setup/
  â”‚     â”œâ”€â”€ </span><span>cluster</span><span>-kind.yaml
  â”‚     â”œâ”€â”€ install-observability.sh
  â”œâ”€â”€ datasets/
  â”‚     â”œâ”€â”€ logs-simulados.</span><span>log</span><span>
  â”‚     â”œâ”€â”€ metrics-export.prom
  â”œâ”€â”€ dashboards/
  â”‚     â”œâ”€â”€ logs-dashboard.json
  â”‚     â”œâ”€â”€ metrics-dashboard.json
  â”œâ”€â”€ agent/
        â”œâ”€â”€ insight_agent.py
        â”œâ”€â”€ prompts/
              â”œâ”€â”€ prompt_incidente.txt
</span></span></code></div></div></pre>

---

## ğŸ”¥ Â¿QuÃ© sigue?

Dime quÃ© quieres construir primero:

1. **Cluster Kind + instalaciÃ³n de Prometheus/Grafana/Loki**
2. **El agente LLM en Python**
3. **Dataset de logs simulados**
4. **Los dashboards JSON**
5. **La presentaciÃ³n en diapositivas**
6. **El repositorio GitHub base (te lo genero)**
